{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "from langchain.agents import tool\n",
    "import search_functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '.'\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "client = chromadb.Client()\n",
    "\n",
    "print(client.list_collections())\n",
    "\n",
    "for i in range(len(client.list_collections())):\n",
    "    if len(client.list_collections()) == 0:\n",
    "        break\n",
    "    client.delete_collection(client.list_collections()[0].name)\n",
    "\n",
    "print(client.list_collections())\n",
    "\n",
    "embedded_chunks_collection = client.get_or_create_collection(\"embedded_chunks\")\n",
    "context_library_collection = client.get_or_create_collection(\"context_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(keywords: str, start_year=1000, end_year=2023) -> int:\n",
    "    \"\"\"Search for papers to increase the paper count. Input should be a string of keywords. Use\n",
    "        this format: [keyword search], [start year]-[end year]. You may include years as the last\n",
    "        word in the query, e.g. 'machine learning 2020' or 'quantum computing ai 2010-2020'. The\n",
    "        current year is 2023.\"\"\"\n",
    "    \n",
    "    links = search_functions.get_arxiv_articles(keywords=keywords)\n",
    "    print(links)\n",
    "    for link in tqdm(links, desc=\"Processing articles\"):\n",
    "        try:\n",
    "            # Get chunks from the arXiv PDF\n",
    "            chunks = search_functions.request_arxiv_pdf_chunks(link)\n",
    "\n",
    "            # Embed the chunks\n",
    "            embedded_chunks = embeddings.embed_documents(chunks[0])\n",
    "\n",
    "            # Add each embedded chunk to the collection\n",
    "            for i, embedded_chunk in enumerate(embedded_chunks):\n",
    "                # Assuming the method expects a dictionary\n",
    "                embedded_chunks_collection.upsert(\n",
    "                    ids=f\"{link}_{i}\",  # Creating a unique ID for each chunk\n",
    "                    documents=chunks[0][i],\n",
    "                    embeddings=embedded_chunk\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "\n",
    "\n",
    "    ###### FIX THIS SO THAT THE ACTUAL AMOUNT IS THE ACTUAL AMOUNT################\n",
    "    return f\"Added {len(links)} new articles to the paper count.\"\n",
    "\n",
    "@tool\n",
    "def gather_evidence(query: str) -> int:\n",
    "    \"\"\"Give a specific question to get evidence for it. This will increase evidence and relevant\n",
    "        paper counts.\"\"\"\n",
    "    \n",
    "    M_results = 5\n",
    "    summary_length = \"750 characters max\"\n",
    "    relevance_score_threshold = 9\n",
    "\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    ###### Figure out how to do MMR ######\n",
    "    ###### MMR ######\n",
    "    ###### MMRR #$$#$#$#$###\n",
    "    results = embedded_chunks_collection.query(\n",
    "        query_embeddings = query_vector,\n",
    "        n_results = M_results\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Summarize the text below to help answer the given question. Do not directly answer the question,\n",
    "        instead summarize the Text so that it gives evidence to answer the question. Reply 'Not applicable' if\n",
    "        text is irrelevant. Use {summary_length}. At the end of your response, provide an integer score from\n",
    "        1-10 on a newline indicating direct relevance to question. Do not explain your score.\n",
    "        (\n",
    "            Directly Relevant Example:\n",
    "            Relevant Information Summary: \"The text...\" (Summarizes the Text chunk in direct relation to the question, highlighting the ways it answers the question)\n",
    "            <SCORE:> 10\n",
    "            \n",
    "            Very Useful Example:\n",
    "            Relevant Information Summary: \"The text...\" (Summarizes the Text chunk in direct relation to the question, highlighting the ways it answers the question)\n",
    "            <SCORE:> 8\n",
    "\n",
    "            Somewhat Relevant Example:\n",
    "            Relevant Information Summary: \"The text...\" (Summarizes the Text chunk in direct relation to the question, pointing out similarities to the question)\n",
    "            <SCORE:> 6\n",
    "\n",
    "            Not Relevant Example:\n",
    "            Relevant Information Summary: \"Not applicable.\"\n",
    "        )\n",
    "        \n",
    "        Text: {chunk}\n",
    "        Excerpt from: {citation}\n",
    "        Question: {question}\n",
    "        Relevant Information Summary:\"\"\"\n",
    "    )\n",
    "        \n",
    "    chain = prompt | llm\n",
    "\n",
    "    if len(results['ids']) > 0:\n",
    "        combined_list = []\n",
    "        for i in range(len(results['ids'])):\n",
    "            response = chain.invoke({\n",
    "                \"summary_length\": \"max 750 characters\",\n",
    "                \"chunk\": results['documents'][0][i],\n",
    "                \"citation\": results['ids'][0][i],\n",
    "                \"question\": query\n",
    "            })\n",
    "\n",
    "            # Extract the score from the response\n",
    "            #print(response.content)\n",
    "            try:\n",
    "                score_str = response.content.split('<SCORE:>')[1].strip()\n",
    "                score = int(score_str)\n",
    "            except:\n",
    "                score = 0\n",
    "\n",
    "            # Add a tuple of (score, summary) to the combined list\n",
    "            combined_list.append((score, response.content, results['ids'][0][i]))\n",
    "\n",
    "        # Sort the list based on scores in descending order\n",
    "        combined_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for score, response, id in combined_list:\n",
    "            if score >= relevance_score_threshold:\n",
    "                context_library_collection.add(\n",
    "                        ids = id,  # Creating a unique ID for each chunk\n",
    "                        ##########documents = results['documents'][0][i],\n",
    "                        documents = response,\n",
    "                        metadatas = {\"score\": score}\n",
    "                ) \n",
    "            \n",
    "        return combined_list[0]\n",
    "    \n",
    "    else:\n",
    "        return \"No queries found\"\n",
    "\n",
    "@tool\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"Ask a model to propose an answer using evidence from papers. The input is the question to\n",
    "    be answered. The tool may fail, indicating that better or different evidence should be\n",
    "    found.\"\"\"\n",
    "\n",
    "    answer_length = \"750 words maximum\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"We are collecting background information for the question/task below.\n",
    "        Provide a brief summary of information you know (about 50 words) that could help answer\n",
    "        the question - do not answer it directly and ignore formatting instructions. If there is \n",
    "        nothing to contribute, do not answer.\n",
    "        Question: {question}\n",
    "        Answer: Here is what I know. \"\"\"\n",
    "    )\n",
    "\n",
    "    ask_llm = prompt | llm\n",
    "\n",
    "    latent_knowledge = \"\"\n",
    "    \"\"\"ask_llm.invoke({\n",
    "            \"question\": question\n",
    "    })\"\"\"\n",
    "\n",
    "    context = \"\"\n",
    "    for i in range(context_library_collection.count()):\n",
    "        context += context_library_collection.get()['documents'][i]\n",
    "        context += \"Citation: \" + context_library_collection.get()['ids'][i][:len(context_library_collection.get()['ids'][i])-2]\n",
    "    \n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Write an answer {answer_length} for the question below based on the provided context. If\n",
    "        the context provides insufficient information, reply 'I cannot answer'. For each part of\n",
    "        your answer, indicate which sources most support it via valid citation markers at the end\n",
    "        of sentences, like (Example 2012). Answer in an unbiased, comprehensive, and scholarly\n",
    "        tone. If the question is subjective, provide an opinionated answer in the concluding 1-2\n",
    "        sentences.\n",
    "        {context}\n",
    "        Extra background information: {latent_knowledge}\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    ask_llm = prompt | llm\n",
    "\n",
    "    answer = ask_llm.invoke({\n",
    "            \"answer_length\": answer_length,\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"latent_knowledge\": latent_knowledge,\n",
    "    })\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search, gather_evidence, answer_question]\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"\"\"Answer question: {question}. Search for papers, gather evidence, and answer. If you do not,\n",
    "            have enough evidence, you can search for more papers (preferred) or gather more evidence\n",
    "            with a different phrase. You may rephrase or break-up the question in those steps. Once\n",
    "            you have five or more pieces of evidence from multiple sources, or you have tried many\n",
    "            times, call answer_question tool. You may reject the answer and try again if it is\n",
    "            incomplete.\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search` with `{'keywords': 'AI systems ethical decisions'}`\n",
      "\n",
      "\n",
      "\u001b[0m[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mAdded 0 new articles to the paper count.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `gather_evidence` with `{'query': 'ethical decision making in AI systems'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m(8, 'The text includes references to various papers and reports related to ethical principles and decision making in AI systems. These include topics such as ethical design, human-agent collectives, personalized persuasion, explainable agency, computational models of ethical reasoning, and voting-based systems for ethical decision making. The text also mentions the challenges and roadblocks to the adoption of self-driving vehicles. Overall, the text provides evidence of research and discussions on ethical decision making in AI systems. \\n\\n<SCORE:> 8', 'http://arxiv.org/abs/1812.02953v1_10')\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `gather_evidence` with `{'query': 'ethical AI decision making frameworks'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: http://arxiv.org/abs/2306.01774v1_3\n",
      "Add of existing embedding ID: http://arxiv.org/abs/2306.01774v1_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m(9, \"The text discusses various ethical AI decision-making frameworks, including the Ethics Guidelines developed by the European Commission's High-level expert group on artificial intelligence (AI HLEG), the responsible research and innovation (RRI) framework, the principles of responsible AI proposed by Crowley et al., and the framework presented by Hallensleben et al. These frameworks emphasize principles such as respect for human autonomy, prevention of harm, fairness, explicability, adaptability, responsibility, and transparency. They also highlight the importance of considering cultural diversity and representation in the development of AI systems to ensure inclusivity, fairness, and trustworthiness. The frameworks provide guidance on incorporating values into algorithmic decision-making, measuring the fulfillment of values, and monitoring ethically relevant system characteristics. \\n<SCORE:> 9\", 'http://arxiv.org/abs/2306.01774v1_3')\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `gather_evidence` with `{'query': 'ethical decision making models for AI systems'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m(7, 'The text includes references to various papers and reports related to ethical principles and decision making in AI systems. These include topics such as ethical design, human-agent collectives, personalized persuasion, explainable agency, computational models of ethical reasoning, and voting-based systems for ethical decision making. The text does not provide specific details about ethical decision making models for AI systems, but it does indicate that there is a body of research and literature on this topic. \\n\\n<SCORE:> 7', 'http://arxiv.org/abs/1812.02953v1_10')\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `gather_evidence` with `{'query': 'ethical considerations in AI decision making'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m(9, \"The text discusses the need for ethical considerations in AI systems. It mentions a specific example of bias in the Amazon recruitment system that led to discriminatory issues. It emphasizes the importance of developing policies and principles to address the ethical aspects of AI systems to protect people's control, safety, livelihood, and rights. The text also mentions that the current state of AI ethics is largely unknown to the public, practitioners, policy, and lawmakers. It highlights the three components that an ethically aligned AI system should meet: compliance with laws and regulations, adherence to ethical principles and values, and technical and social robustness. The text mentions the lack of empirical studies exploring AI ethics principles and challenges and presents three research questions to address this gap. The text then describes the methodology of a survey study conducted to gather the insights and opinions of practitioners and lawmakers on AI ethics principles and challenges. The background section briefly explains that AI ethics is a branch of applied ethics that focuses on ethical issues associated with developing and using AI systems. \\n\\n<SCORE:> 9\", 'http://arxiv.org/abs/2207.01493v2_1')\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `gather_evidence` with `{'query': 'ethical decision making algorithms for AI systems'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m(8, 'The text includes references to various papers and reports related to ethical principles and decision-making in AI systems. It mentions topics such as ethical design, human-agent collectives, personalized persuasion, explainable agency, computational models of ethical reasoning, and voting-based systems for ethical decision making. These references provide evidence of research and discussions on ethical decision-making algorithms for AI systems. \\n\\n<SCORE:> 8', 'http://arxiv.org/abs/1812.02953v1_10')\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_question` with `{'question': 'How can we ensure AI systems make ethical decisions?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mcontent=\"Ensuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's High-level expert group on AI has identified four ethical principles that can guide AI decision-making: respect for human autonomy, prevention of harm, fairness, and explicability. These principles emphasize the importance of ensuring that AI systems respect human values, do not cause harm, treat individuals fairly, and are transparent and explainable in their decision-making processes (European Commission 2019).\\n\\nIn addition to these principles, there are seven key requirements for achieving ethical AI, as mentioned in the text. These requirements include accountability, data governance, transparency, diversity, non-discrimination, societal and environmental well-being, and robustness. These requirements highlight the need for AI systems to be accountable for their actions, to handle data responsibly, to be transparent in their decision-making processes, to consider cultural diversity and representation, to avoid discrimination, to contribute to societal and environmental well-being, and to be technically and socially robust (European Commission 2019).\\n\\nCultural diversity and representation are crucial factors to consider in the development of AI systems. The text emphasizes the importance of avoiding biases and ensuring that AI systems are inclusive and fair. The responsible research and innovation (RRI) framework is mentioned as an effective way to achieve inclusivity and fairness. The RRI framework encourages the involvement of diverse stakeholders in the design and development of AI systems, ensuring that different perspectives and values are taken into account (Stilgoe et al. 2013).\\n\\nThe principles of responsible AI presented by Crowley et al. also contribute to ethical decision-making in AI systems. These principles include adaptability, responsibility, and transparency. Adaptability refers to the ability of AI systems to learn and evolve based on feedback and changing circumstances. Responsibility emphasizes the need for AI systems to be accountable for their actions and to consider the potential impacts on individuals and society. Transparency highlights the importance of making AI systems explainable and understandable to users and stakeholders (Crowley et al. 2019).\\n\\nThe Design for Values (DFV) methodology is discussed as a structured approach for formulating requirements in AI systems. This methodology involves identifying and incorporating human values into the design process, ensuring that AI systems align with ethical principles and societal norms. The DFV methodology helps to address the ethical considerations and potential biases in AI systems by explicitly considering the values and preferences of different stakeholders (van den Hoven et al. 2013).\\n\\nHallensleben et al. present a framework for incorporating ethical principles into the design, implementation, and evaluation of AI systems. This framework emphasizes the need for a multidisciplinary approach, involving experts from various fields such as ethics, law, and social sciences. It also highlights the importance of considering the potential impacts of AI systems on individuals, society, and the environment. By integrating ethical principles throughout the entire lifecycle of AI systems, this framework aims to ensure that ethical considerations are not an afterthought but an integral part of the design process (Hallensleben et al. 2020).\\n\\nIn conclusion, ensuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's ethical principles, the seven key requirements for ethical AI, the responsible research and innovation framework, the principles of responsible AI, the Design for Values methodology, and the framework presented by Hallensleben et al. are all valuable resources for guiding ethical AI decision-making. By considering these frameworks and principles, AI systems can be developed and used in a way that respects human values, prevents harm, promotes fairness, and ensures transparency and accountability. However, it is important to note that the field of AI ethics is still evolving, and further research and empirical studies are needed to address the challenges and complexities associated with ethical AI decision-making (Example 2012).\"\u001b[0m\u001b[32;1m\u001b[1;3mEnsuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's High-level expert group on AI has identified four ethical principles that can guide AI decision-making: respect for human autonomy, prevention of harm, fairness, and explicability. These principles emphasize the importance of ensuring that AI systems respect human values, do not cause harm, treat individuals fairly, and are transparent and explainable in their decision-making processes (European Commission 2019).\n",
      "\n",
      "In addition to these principles, there are seven key requirements for achieving ethical AI, as mentioned in the text. These requirements include accountability, data governance, transparency, diversity, non-discrimination, societal and environmental well-being, and robustness. These requirements highlight the need for AI systems to be accountable for their actions, to handle data responsibly, to be transparent in their decision-making processes, to consider cultural diversity and representation, to avoid discrimination, to contribute to societal and environmental well-being, and to be technically and socially robust (European Commission 2019).\n",
      "\n",
      "Cultural diversity and representation are crucial factors to consider in the development of AI systems. The text emphasizes the importance of avoiding biases and ensuring that AI systems are inclusive and fair. The responsible research and innovation (RRI) framework is mentioned as an effective way to achieve inclusivity and fairness. The RRI framework encourages the involvement of diverse stakeholders in the design and development of AI systems, ensuring that different perspectives and values are taken into account (Stilgoe et al. 2013).\n",
      "\n",
      "The principles of responsible AI presented by Crowley et al. also contribute to ethical decision-making in AI systems. These principles include adaptability, responsibility, and transparency. Adaptability refers to the ability of AI systems to learn and evolve based on feedback and changing circumstances. Responsibility emphasizes the need for AI systems to be accountable for their actions and to consider the potential impacts on individuals and society. Transparency highlights the importance of making AI systems explainable and understandable to users and stakeholders (Crowley et al. 2019).\n",
      "\n",
      "The Design for Values (DFV) methodology is discussed as a structured approach for formulating requirements in AI systems. This methodology involves identifying and incorporating human values into the design process, ensuring that AI systems align with ethical principles and societal norms. The DFV methodology helps to address the ethical considerations and potential biases in AI systems by explicitly considering the values and preferences of different stakeholders (van den Hoven et al. 2013).\n",
      "\n",
      "Hallensleben et al. present a framework for incorporating ethical principles into the design, implementation, and evaluation of AI systems. This framework emphasizes the need for a multidisciplinary approach, involving experts from various fields such as ethics, law, and social sciences. It also highlights the importance of considering the potential impacts of AI systems on individuals, society, and the environment. By integrating ethical principles throughout the entire lifecycle of AI systems, this framework aims to ensure that ethical considerations are not an afterthought but an integral part of the design process (Hallensleben et al. 2020).\n",
      "\n",
      "In conclusion, ensuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's ethical principles, the seven key requirements for ethical AI, the responsible research and innovation framework, the principles of responsible AI, the Design for Values methodology, and the framework presented by Hallensleben et al. are all valuable resources for guiding ethical AI decision-making. By considering these frameworks and principles, AI systems can be developed and used in a way that respects human values, prevents harm, promotes fairness, and ensures transparency and accountability. However, it is important to note that the field of AI ethics is still evolving, and further research and empirical studies are needed to address the challenges and complexities associated with ethical AI decision-making.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"How can we ensure AI systems make ethical decisions?\"\n",
    "result = agent_executor.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "#agent_executor.invoke({\"question\": \"are these real words?\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ensuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's High-level expert group on AI has identified four ethical principles that can guide AI decision-making: respect for human autonomy, prevention of harm, fairness, and explicability. These principles emphasize the importance of ensuring that AI systems respect human values, do not cause harm, treat individuals fairly, and are transparent and explainable in their decision-making processes (European Commission 2019).\\n\\nIn addition to these principles, there are seven key requirements for achieving ethical AI, as mentioned in the text. These requirements include accountability, data governance, transparency, diversity, non-discrimination, societal and environmental well-being, and robustness. These requirements highlight the need for AI systems to be accountable for their actions, to handle data responsibly, to be transparent in their decision-making processes, to consider cultural diversity and representation, to avoid discrimination, to contribute to societal and environmental well-being, and to be technically and socially robust (European Commission 2019).\\n\\nCultural diversity and representation are crucial factors to consider in the development of AI systems. The text emphasizes the importance of avoiding biases and ensuring that AI systems are inclusive and fair. The responsible research and innovation (RRI) framework is mentioned as an effective way to achieve inclusivity and fairness. The RRI framework encourages the involvement of diverse stakeholders in the design and development of AI systems, ensuring that different perspectives and values are taken into account (Stilgoe et al. 2013).\\n\\nThe principles of responsible AI presented by Crowley et al. also contribute to ethical decision-making in AI systems. These principles include adaptability, responsibility, and transparency. Adaptability refers to the ability of AI systems to learn and evolve based on feedback and changing circumstances. Responsibility emphasizes the need for AI systems to be accountable for their actions and to consider the potential impacts on individuals and society. Transparency highlights the importance of making AI systems explainable and understandable to users and stakeholders (Crowley et al. 2019).\\n\\nThe Design for Values (DFV) methodology is discussed as a structured approach for formulating requirements in AI systems. This methodology involves identifying and incorporating human values into the design process, ensuring that AI systems align with ethical principles and societal norms. The DFV methodology helps to address the ethical considerations and potential biases in AI systems by explicitly considering the values and preferences of different stakeholders (van den Hoven et al. 2013).\\n\\nHallensleben et al. present a framework for incorporating ethical principles into the design, implementation, and evaluation of AI systems. This framework emphasizes the need for a multidisciplinary approach, involving experts from various fields such as ethics, law, and social sciences. It also highlights the importance of considering the potential impacts of AI systems on individuals, society, and the environment. By integrating ethical principles throughout the entire lifecycle of AI systems, this framework aims to ensure that ethical considerations are not an afterthought but an integral part of the design process (Hallensleben et al. 2020).\\n\\nIn conclusion, ensuring that AI systems make ethical decisions requires the implementation of various frameworks, principles, and methodologies. The European Commission's ethical principles, the seven key requirements for ethical AI, the responsible research and innovation framework, the principles of responsible AI, the Design for Values methodology, and the framework presented by Hallensleben et al. are all valuable resources for guiding ethical AI decision-making. By considering these frameworks and principles, AI systems can be developed and used in a way that respects human values, prevents harm, promotes fairness, and ensures transparency and accountability. However, it is important to note that the field of AI ethics is still evolving, and further research and empirical studies are needed to address the challenges and complexities associated with ethical AI decision-making.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
